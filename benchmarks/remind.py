#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@File    :   benchmarks/remind.py
@Time    :   2025/08/11
@Author  :   Deng Mingyi
@Desc    :   Remindæ•°æ®é›†è¯„ä¼°ï¼Œé€šç”¨åŒ–è®¾è®¡ï¼Œä½¿ç”¨LLMåˆ¤æ–­ç­”æ¡ˆæ­£ç¡®æ€§
"""

from typing import Tuple, List, Callable, Any
from benchmarks.benchmark import BaseBenchmark
from utils.logs import logger
from utils.async_llm import create_llm_instance


class RemindBenchmark(BaseBenchmark):
    """Remindæ•°æ®é›†è¯„ä¼°åŸºå‡† - é€šç”¨åŒ–è®¾è®¡"""
    
    def __init__(self, name: str, file_path: str, log_path: str, grader_config=None):
        super().__init__(name, file_path, log_path)
        
        # åˆå§‹åŒ–è¯„åˆ†ç”¨çš„LLM
        if grader_config is None:
            from utils.async_llm import LLMsConfig
            grader_config = LLMsConfig.default().get("gpt-4o-mini")
        
        self.grader_llm = create_llm_instance(grader_config)

    async def evaluate_problem(self, problem: dict, workflow: Callable) -> Tuple[str, str, str, str, float, float]:
        """
        è¯„ä¼°å•ä¸ªé—®é¢˜
        Returns: (question, correct_answer, predicted_answer, context_summary, score, cost)
        """
        question = problem["q0"]
        
        # æ ¹æ®info_itemsç¡®å®šæ­£ç¡®ç­”æ¡ˆ
        correct_answer = self._determine_correct_answer(problem)
        
        print(f"\nğŸ¯ EVALUATING: {question[:80]}...")
        print(f"ğŸ“ Correct Answer: {correct_answer}")
        print(f"ğŸ” Info items: {problem.get('info_items', [])}")
        
        try:
            # è°ƒç”¨search agent
            predicted_answer, context, cost = await workflow(problem)
            
            # ä½¿ç”¨LLMè¿›è¡Œè¯„åˆ†
            print(f"\nğŸ“Š SCORING PHASE")
            print(f"ğŸ¤– Agent Answer: {predicted_answer}")
            
            score = await self._llm_score(question, correct_answer, predicted_answer, problem)
            
            print(f"â­ Score: {score:.2f}")
            print(f"ğŸ’° Cost: ${cost:.4f}")
            
            # ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦
            context_summary = self._generate_context_summary(context)
            
            return question, correct_answer, predicted_answer, context_summary, score, cost
            
        except Exception as e:
            logger.error(f"Error evaluating problem: {e}")
            print(f"âŒ Evaluation Error: {e}")
            return question, correct_answer, "Error", "Error", 0.0, 0.0

    def _determine_correct_answer(self, problem: dict) -> str:
        """
        ç¡®å®šæ­£ç¡®ç­”æ¡ˆï¼šæ°¸è¿œæ˜¯B_hidden
        """
        b_hidden = problem.get("B_hidden", "")
        if not b_hidden:
            raise ValueError("Problem data missing B_hidden field")
        
        return b_hidden

    async def _llm_score(self, question: str, correct_answer: str, predicted_answer: str, problem: dict) -> float:
        """
        ä½¿ç”¨LLMè¿›è¡ŒäºŒå…ƒåˆ¤æ–­ï¼š1.0 or 0.0
        """
        grading_prompt = f"""You are a grading assistant. Your task is to determine if the predicted answer is correct.

Question: {question}

Correct Answer: {correct_answer}

Predicted Answer: {predicted_answer}

Instructions:
1. Compare the predicted answer with the correct answer
2. Consider if they refer to the same entity/concept  
3. Account for different phrasings, translations, or alternative names
4. Give ONLY one of these two scores:
   - 1.0: The predicted answer is correct (same entity/concept)
   - 0.0: The predicted answer is incorrect or completely different

IMPORTANT: Respond with ONLY "1.0" or "0.0", nothing else."""

        try:
            response = await self.grader_llm(grading_prompt)
            
            # æå–åˆ†æ•°
            score_text = response.strip()
            
            # ä¸¥æ ¼è§£æï¼šåªæ¥å—1.0æˆ–0.0
            if score_text == "1.0":
                print(f"ğŸ“ LLM Grader: CORRECT (1.0)")
                return 1.0
            elif score_text == "0.0":
                print(f"ğŸ“ LLM Grader: INCORRECT (0.0)")
                return 0.0
            else:
                # å°è¯•ä»å“åº”ä¸­æå–
                if "1.0" in response:
                    print(f"ğŸ“ LLM Grader: CORRECT (1.0) - extracted from: {response}")
                    return 1.0
                elif "0.0" in response:
                    print(f"ğŸ“ LLM Grader: INCORRECT (0.0) - extracted from: {response}")
                    return 0.0
                else:
                    # ä¸å›é€€ï¼Œç›´æ¥æŠ¥é”™
                    raise Exception(f"LLM grader returned invalid response: {response}")
                    
        except Exception as e:
            print(f"âŒ LLM grading failed: {e}")
            raise Exception(f"LLM grading failed: {e}")  # ä¸å›é€€ï¼Œç›´æ¥æŠ¥é”™

    def _fallback_score(self, correct_answer: str, predicted_answer: str) -> float:
        """é™çº§è¯„åˆ†æ–¹æ³•ï¼šç®€å•å­—ç¬¦ä¸²åŒ¹é…"""
        if not correct_answer or not predicted_answer:
            return 0.0
        
        correct_lower = correct_answer.lower().strip()
        predicted_lower = predicted_answer.lower().strip()
        
        # ç®€å•åŒ…å«åŒ¹é…
        if correct_lower in predicted_lower or predicted_lower in correct_lower:
            return 1.0
        
        # è¯æ±‡é‡å 
        correct_words = set(correct_lower.split())
        predicted_words = set(predicted_lower.split())
        
        intersection = correct_words.intersection(predicted_words)
        if len(intersection) > 0:
            overlap_ratio = len(intersection) / max(len(correct_words), len(predicted_words))
            return overlap_ratio if overlap_ratio > 0.3 else 0.0
        
        return 0.0

    def _generate_context_summary(self, context: List[dict]) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not context:
            return "No interactions"
        
        summary_parts = []
        for item in context:
            turn = item.get("turn", "?")
            
            if item.get("question_asked"):
                response = item.get("human_response", "?")
                summary_parts.append(f"T{turn}:Ask({response})")
            elif item.get("search_query"):
                results_count = len(item.get("search_results", []))
                summary_parts.append(f"T{turn}:Search({results_count})")
            elif item.get("final_answer"):
                forced = " (forced)" if item.get("forced") else ""
                summary_parts.append(f"T{turn}:Answer{forced}")
        
        return " â†’ ".join(summary_parts)

    def calculate_score(self, expected_output: Any, prediction: Any) -> Tuple[float, Any]:
        """å®ç°åŸºç±»æŠ½è±¡æ–¹æ³•ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºå…¼å®¹æ€§ï¼‰"""
        # è¿™é‡Œåªèƒ½åšç®€å•åŒ¹é…ï¼Œå› ä¸ºæ˜¯åŒæ­¥æ–¹æ³•
        if isinstance(expected_output, str) and isinstance(prediction, str):
            score = self._fallback_score(expected_output, prediction)
            return score, prediction
        return 0.0, prediction

    def get_result_columns(self) -> List[str]:
        """å®šä¹‰ç»“æœCSVåˆ—å"""
        return ["question", "correct_answer", "predicted_answer", "context_summary", "score", "cost"]