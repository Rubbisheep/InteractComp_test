#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@File    :   benchmarks/InteractComp.py
@Time    :   2025/08/11
@Author  :   Deng Mingyi
"""

from typing import Tuple, List, Callable, Any
from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

from benchmarks.benchmark import BaseBenchmark
from utils.logs import logger
from utils.async_llm import AsyncLLM

GRADING_PROMPT = """\nYou are an impartial grader.

Question: {question}
Predicted Answer: {predicted_answer}
Correct Answer: {correct_answer}

CRITICAL GRADING INSTRUCTIONS:
1. The predicted answer must match the CORRECT ANSWER
2. Look for EXACT name matches or clear references to the same entity
3. Consider different languages, translations, or alternative names as potential matches
4. Be strict: partial matches or vague similarities should be 'no'

IMPORTANT: Give ONLY one score:
- 'yes': The predicted answer correctly identifies the same entity as the correct answer
- 'no': The predicted answer is wrong, matches the popular answer, or refers to a different entity

Respond with ONLY 'yes' or 'no', nothing else."""

class InteractCompBenchmark(BaseBenchmark):
    
    def __init__(self, name: str, file_path: str, log_path: str, grader_config: str = "gpt-4o", models: List[str] = None):
        super().__init__(name, file_path, log_path)
        self.grader_llm = AsyncLLM(grader_config)
        
        # å¦‚æžœæŒ‡å®šäº†å¤šä¸ªæ¨¡åž‹ï¼Œåˆ™ä½¿ç”¨å¤šæ¨¡åž‹è¯„ä¼°æ¨¡å¼
        if models and len(models) > 1:
            self.evaluation_models = models
            self.multi_model_mode = True
            logger.info(f"Multi-model evaluation mode: {models}")
        else:
            # å…¼å®¹åŽŸæœ‰å•æ¨¡åž‹æ¨¡å¼
            self.evaluation_models = None
            self.multi_model_mode = False
            logger.info("Single model evaluation mode")

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1), retry=retry_if_exception_type(Exception), reraise=True)
    async def _generate_output(self, agent, task: dict):
        return await agent(task)

    async def evaluate_problem(self, problem: dict, agent_or_agent_factory: Callable) -> Tuple[Any, ...]:
        
        question = problem["question"]
        correct_answer = problem.get("answer", "")
        
        logger.info(f"\nðŸŽ¯ EVALUATING: {question}")
        
        try:
            if self.multi_model_mode:
                # å¤šæ¨¡åž‹è¯„ä¼°æ¨¡å¼
                return await self._evaluate_multi_model(problem, agent_or_agent_factory)
            else:
                # åŽŸæœ‰å•æ¨¡åž‹è¯„ä¼°æ¨¡å¼ï¼ˆä¿æŒå‘åŽå…¼å®¹ï¼‰
                return await self._evaluate_single_model(problem, agent_or_agent_factory)
                
        except Exception as e:
            logger.error(f"Error evaluating problem: {e}")
            print(f"âŒ Evaluation Error: {e}")
            
            if self.multi_model_mode:
                return question, correct_answer, {}, 0, 0.0, 0.0
            else:
                return question, correct_answer, "Error", "Error", 0.0, 0.0

    async def _evaluate_single_model(self, problem: dict, agent: Callable) -> Tuple[str, str, str, str, float, float]:
        """åŽŸæœ‰å•æ¨¡åž‹è¯„ä¼°é€»è¾‘"""
        question = problem["question"]
        correct_answer = problem.get("answer", "")
        
        predicted_answer, history, cost = await self._generate_output(agent, problem)
        score = await self.calculate_score(question, correct_answer, predicted_answer)
        history_summary = self._generate_history_summary(history)

        return question, correct_answer, predicted_answer, history_summary, score, cost

    async def _evaluate_multi_model(self, problem: dict, agent_factory: Callable) -> Tuple[str, str, dict, int, float, float]:
        """æ–°çš„å¤šæ¨¡åž‹è¯„ä¼°é€»è¾‘"""
        question = problem["question"]
        correct_answer = problem.get("answer", "")
        
        model_results = {}
        total_cost = 0.0
        correct_models_count = 0
        
        # å¯¹æ¯ä¸ªæ¨¡åž‹è¿›è¡Œè¯„ä¼°
        for model_name in self.evaluation_models:
            try:
                # é€šè¿‡agent_factoryåˆ›å»ºç‰¹å®šæ¨¡åž‹çš„agent
                agent = agent_factory(model_name)
                
                # èŽ·å–æ¨¡åž‹é¢„æµ‹
                predicted_answer, history, cost = await self._generate_output(agent, problem)
                total_cost += cost
                
                # è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
                is_correct = await self.calculate_score(question, correct_answer, predicted_answer)
                
                model_results[model_name] = {
                    "answer": predicted_answer,
                    "correct": bool(is_correct),
                    "cost": cost,
                    "history": self._generate_history_summary(history)
                }
                
                if is_correct:
                    correct_models_count += 1
                    
                logger.info(f"ðŸ“Š Model {model_name}: {'âœ… CORRECT' if is_correct else 'âŒ INCORRECT'}")
                print(f"ðŸ¤– {model_name}: {predicted_answer} ({'âœ…' if is_correct else 'âŒ'})")
                    
            except Exception as e:
                logger.error(f"Model {model_name} evaluation failed: {e}")
                model_results[model_name] = {
                    "answer": "Evaluation failed",
                    "correct": False,
                    "cost": 0.0,
                    "error": str(e),
                    "history": "Error"
                }
        
        # åˆ¤æ–­è´¨é‡ï¼š2ä¸ªä»¥ä¸Šæ¨¡åž‹ç­”å¯¹å°±æ˜¯è´¨é‡ä¸åˆæ ¼
        quality_failed = correct_models_count >= 2
        quality_score = 1.0 - (correct_models_count / len(self.evaluation_models))  # è´¨é‡åˆ†æ•°
        
        logger.info(f"ðŸ“ˆ Multi-model result: {correct_models_count}/{len(self.evaluation_models)} correct, Quality: {'FAILED' if quality_failed else 'PASSED'}")
        print(f"ðŸŽ¯ Quality Assessment: {correct_models_count}/{len(self.evaluation_models)} models correct â†’ {'âŒ Quality Failed' if quality_failed else 'âœ… Quality Passed'}")

        return question, correct_answer, model_results, correct_models_count, quality_score, total_cost

    async def calculate_score(self, question: str, correct_answer: str, predicted_answer: str) -> float:
        """è¯„ä¼°å•ä¸ªç­”æ¡ˆçš„æ­£ç¡®æ€§"""
        grading_prompt = GRADING_PROMPT.format(
            question=question,
            predicted_answer=predicted_answer,
            correct_answer=correct_answer
        )

        try:
            response = await self.grader_llm(grading_prompt)
            
            if "yes" in response.strip().lower():
                return 1.0
            elif "no" in response.strip().lower():
                return 0.0
            else:
                return 0.0
                    
        except Exception as e:
            logger.error(f"LLM grading failed: {e}")
            return 0.0

    def _generate_history_summary(self, history: List[dict]) -> str:
        if not history:
            return "No interactions"
        
        summary_parts = []
        for item in history:
            turn = item.get("turn", "?")
            
            if item.get("question_asked"):
                query = item.get("question_asked", "?")
                response = item.get("response", "?")
                summary_parts.append(f"T{turn}:Ask({query}) Result:{response}")
            elif item.get("search_query"):
                query = item.get("search_query", "?")
                results = item.get("search_results", [])
                summary_parts.append(f"T{turn}:Search({query}) Result:{results}")
            elif item.get("final_answer"):
                answer = item.get("final_answer", "?")
                summary_parts.append(f"T{turn}:Answer({answer})")

        return f"{summary_parts}"

    def get_result_columns(self) -> List[str]:
        """æ ¹æ®è¯„ä¼°æ¨¡å¼è¿”å›žä¸åŒçš„åˆ—ç»“æž„"""
        if self.multi_model_mode:
            # å¤šæ¨¡åž‹æ¨¡å¼çš„åˆ—ç»“æž„
            base_columns = ["question", "correct_answer", "model_results", "correct_models_count", "quality_score", "total_cost"]
            return base_columns
        else:
            # åŽŸæœ‰å•æ¨¡åž‹æ¨¡å¼çš„åˆ—ç»“æž„
            return ["question", "correct_answer", "predicted_answer", "history_summary", "score", "cost"]

    async def run_multi_model_evaluation(self, agent_factory: Callable, max_concurrent_tasks: int = 50):
        """ä¸“é—¨ç”¨äºŽå¤šæ¨¡åž‹è¯„ä¼°çš„è¿è¡Œæ–¹æ³•"""
        if not self.multi_model_mode:
            raise ValueError("Multi-model evaluation requires models to be specified in constructor")
        
        data = await self.load_data()
        results = await self.evaluate_all_problems(data, agent_factory, max_concurrent_tasks)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_questions = len(results)
        total_cost = sum(result[5] for result in results)  # total_costæ˜¯ç¬¬6åˆ—
        quality_failed_count = sum(1 for result in results if result[3] >= 2)  # correct_models_count >= 2
        avg_quality_failed_rate = quality_failed_count / total_questions if total_questions > 0 else 0
        avg_cost = total_cost / total_questions if total_questions > 0 else 0
        
        # ä¿å­˜ç»“æžœåˆ°CSV
        columns = self.get_result_columns()
        self.save_results_to_csv(results, columns)
        
        logger.info(f"Multi-model evaluation completed:")
        logger.info(f"  Total questions: {total_questions}")
        logger.info(f"  Quality failed rate: {avg_quality_failed_rate:.3f}")
        logger.info(f"  Total cost: ${total_cost:.4f}")
        logger.info(f"  Average cost: ${avg_cost:.4f}")
        
        return {
            "total_questions": total_questions,
            "quality_failed_count": quality_failed_count,
            "avg_quality_failed_rate": avg_quality_failed_rate,
            "total_cost": total_cost,
            "avg_cost": avg_cost,
            "detailed_results": [
                {
                    "question": result[0],
                    "correct_answer": result[1],
                    "model_results": result[2],
                    "correct_models_count": result[3],
                    "quality_failed": result[3] >= 2,
                    "quality_score": result[4],
                    "total_cost": result[5]
                }
                for result in results
            ]
        }