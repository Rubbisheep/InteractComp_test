#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç®€å•çš„FastAPIåŒ…è£…å™¨ - åŸºäºç°æœ‰çš„InteractCompAgentå’ŒInteractCompBenchmark
"""

import asyncio
import json
import os
import tempfile
import uuid
import csv
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import shutil

from fastapi import FastAPI, File, HTTPException, UploadFile, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel

# å¯¼å…¥ç°æœ‰çš„æ¨¡å—
from benchmarks.InteractComp import InteractCompBenchmark
from workflow.InteractComp import InteractCompAgent
from utils.logs import logger

app = FastAPI(title="InteractCompæ ‡æ³¨è´¨é‡æµ‹è¯•API", version="1.0.0")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚æ¨¡å‹
class TestConfig(BaseModel):
    llm_config: str = "gpt-4o"  # å¯¹åº”ä½ ä»£ç ä¸­çš„llm_configå‚æ•°
    user_config: str = "gpt-4o"  # å¯¹åº”ä½ ä»£ç ä¸­çš„user_configå‚æ•°  
    api_key: str
    base_url: str = "https://api.openai.com/v1"
    max_turns: int = 5
    search_engine_type: str = "llm_knowledge"  # llm_knowledge, google, wikipedia
    max_concurrent_tasks: int = 1

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨
tasks: Dict[str, dict] = {}
uploaded_files: Dict[str, str] = {}

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
    return {
        "message": "InteractCompæ ‡æ³¨æ•°æ®è´¨é‡æµ‹è¯•API",
        "version": "1.0.0",
        "status": "running"
    }

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ JSONLæ•°æ®æ–‡ä»¶"""
    if not file.filename.endswith(('.jsonl', '.json')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ.jsonlå’Œ.jsonæ ¼å¼æ–‡ä»¶")
    
    file_id = str(uuid.uuid4())
    temp_dir = Path("temp_uploads")
    temp_dir.mkdir(exist_ok=True)
    
    file_path = temp_dir / f"{file_id}_{file.filename}"
    
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    uploaded_files[file_id] = str(file_path)
    
    logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename}, ID: {file_id}")
    
    return {
        "file_id": file_id,
        "filename": file.filename,
        "size": file_path.stat().st_size,
        "status": "uploaded"
    }

@app.post("/test/start")
async def start_test(
    file_ids: List[str],
    config: TestConfig,
    background_tasks: BackgroundTasks
):
    """å¼€å§‹æµ‹è¯•ä»»åŠ¡"""
    # éªŒè¯æ–‡ä»¶
    for file_id in file_ids:
        if file_id not in uploaded_files:
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶{file_id}ä¸å­˜åœ¨")
    
    # åˆ›å»ºä»»åŠ¡
    task_id = str(uuid.uuid4())
    task_info = {
        "task_id": task_id,
        "status": "pending",
        "progress": 0,
        "created_at": datetime.now().isoformat(),
        "config": config.dict(),
        "file_ids": file_ids
    }
    tasks[task_id] = task_info
    
    # å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(run_test_with_existing_code, task_id, file_ids, config)
    
    logger.info(f"æµ‹è¯•ä»»åŠ¡å¯åŠ¨: {task_id}")
    
    return {"task_id": task_id, "status": "started"}

@app.get("/test/{task_id}")
async def get_test_status(task_id: str):
    """è·å–æµ‹è¯•çŠ¶æ€"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return tasks[task_id]

@app.get("/test/{task_id}/download-csv")
async def download_csv_report(task_id: str):
    """ä¸‹è½½CSVæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="æµ‹è¯•æœªå®Œæˆ")
    
    # æŸ¥æ‰¾ç”Ÿæˆçš„CSVæ–‡ä»¶
    log_path = f"workspace/web_test_{task_id}/"
    csv_files = list(Path(log_path).glob("*.csv"))
    
    if csv_files:
        # è¿”å›æœ€æ–°çš„CSVæ–‡ä»¶
        latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
        return FileResponse(
            path=str(latest_csv),
            filename=f"interactcomp_test_results_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            media_type='text/csv'
        )
    else:
        # å¦‚æœæ²¡æœ‰CSVæ–‡ä»¶ï¼Œç”Ÿæˆä¸€ä¸ªç®€å•çš„CSV
        import csv
        import tempfile
        
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8')
        
        writer = csv.writer(temp_file)
        writer.writerow(['question', 'correct_answer', 'predicted_answer', 'score', 'cost'])
        
        # ä»failed_itemså†™å…¥æ•°æ®
        for item in task.get("failed_items", []):
            writer.writerow([
                item.get("question", ""),
                item.get("expected_answer", ""),
                item.get("actual_answer", ""),
                0.0,  # å¤±è´¥é¡¹ç›®å¾—åˆ†ä¸º0
                task.get("average_cost", 0.0)
            ])
        
        temp_file.close()
        
        return FileResponse(
            path=temp_file.name,
            filename=f"interactcomp_test_results_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            media_type='text/csv'
        )

@app.get("/test/{task_id}/download")
async def download_report(task_id: str):
    """ä¸‹è½½JSONæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Šï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="æµ‹è¯•æœªå®Œæˆ")
    
    # åˆ›å»ºä¸´æ—¶æŠ¥å‘Šæ–‡ä»¶
    report = {
        "task_id": task_id,
        "results": task,
        "generated_at": datetime.now().isoformat()
    }
    
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
    json.dump(report, temp_file, ensure_ascii=False, indent=2)
    temp_file.close()
    
    return FileResponse(
        path=temp_file.name,
        filename=f"interactcomp_test_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        media_type='application/json'
    )

async def run_test_with_existing_code(task_id: str, file_ids: List[str], config: TestConfig):
    """ä½¿ç”¨ç°æœ‰çš„InteractCompAgentå’ŒInteractCompBenchmarkè¿è¡Œæµ‹è¯•"""
    task = tasks[task_id]
    
    try:
        task["status"] = "running"
        task["progress"] = 10
        
        # 1. æ›´æ–°é…ç½®æ–‡ä»¶ (åŸºäºä½ çš„config2.yamlæ ¼å¼)
        await update_config_file(config)
        task["progress"] = 20
        
        # 2. åˆå¹¶ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶
        combined_file_path = await merge_uploaded_files(file_ids, task_id)
        task["progress"] = 30

        # 3. åˆ›å»ºInteractCompAgentå®ä¾‹ (ä½¿ç”¨ä½ çš„ä»£ç ç»“æ„)
        agent = InteractCompAgent(
            name="WebTest",
            llm_config=config.llm_config,
            dataset="InteractComp",
            prompt="",  # ä½ çš„ä»£ç ä¸­promptä¸ºç©ºå­—ç¬¦ä¸²
            max_turns=config.max_turns,
            search_engine_type=config.search_engine_type,
            user_config=config.user_config
        )
        task["progress"] = 40

        # 4. åˆ›å»ºInteractCompBenchmarkå®ä¾‹
        log_path = f"workspace/web_test_{task_id}/"
        os.makedirs(log_path, exist_ok=True)

        benchmark = InteractCompBenchmark(
            name=f"WebTest_{task_id}",
            file_path=combined_file_path,
            log_path=log_path,
            grader_config=config.user_config
        )
        task["progress"] = 50
        
        # 5. æ‰§è¡Œæµ‹è¯• (ç›´æ¥è°ƒç”¨ä½ çš„run_baselineæ–¹æ³•)
        logger.info(f"å¼€å§‹æ‰§è¡ŒInteractCompåŸºå‡†æµ‹è¯•: {task_id}")

        average_score, average_cost, total_cost = await benchmark.run_baseline(
            agent,
            max_concurrent_tasks=config.max_concurrent_tasks
        )
        
        task["progress"] = 90
        
        # 6. å¤„ç†ç»“æœ
        # è®¡ç®—æˆåŠŸ/å¤±è´¥æ•°é‡ (åŸºäºä½ çš„é€»è¾‘ï¼šå¥½çš„æ ‡æ³¨=æ¨¡å‹ç­”é”™)
        # è¯»å–æ•°æ®æ–‡ä»¶è·å–æ€»é—®é¢˜æ•°
        total_questions = 0
        with open(combined_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    total_questions += 1
        
        successful_tests = int(average_score * total_questions)  # æ¨¡å‹ç­”é”™çš„æ•°é‡
        failed_tests = total_questions - successful_tests  # æ¨¡å‹ç­”å¯¹çš„æ•°é‡(éœ€è¦æ”¹è¿›)
        
        # 7. è¯»å–å¤±è´¥æ—¥å¿— (æ¨¡å‹ç­”å¯¹çš„æ¡ˆä¾‹)
        failed_items = []
        log_file = Path(log_path) / "log.json"
        if log_file.exists():
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    failed_logs = json.load(f)
                    for item in failed_logs[:10]:  # æœ€å¤š10ä¸ª
                        failed_items.append({
                            "question": item.get("question", ""),
                            "expected_answer": item.get("right_answer", ""),
                            "actual_answer": item.get("extracted_output", ""),
                            "reason": "æ¨¡å‹æ‰¾åˆ°äº†æ­£ç¡®ç­”æ¡ˆï¼Œå»ºè®®å¢åŠ é—®é¢˜éš¾åº¦"
                        })
            except Exception as e:
                logger.error(f"è¯»å–æ—¥å¿—å¤±è´¥: {e}")
        
        # 8. æ›´æ–°ä»»åŠ¡ç»“æœ
        task.update({
            "status": "completed",
            "progress": 100,
            "total_questions": total_questions,
            "successful_tests": successful_tests,  # å¥½çš„æ ‡æ³¨æ•°é‡
            "failed_tests": failed_tests,  # éœ€è¦æ”¹è¿›çš„æ•°é‡
            "average_score": average_score,  # æ ‡æ³¨è´¨é‡åˆ†
            "average_cost": average_cost,
            "total_cost": total_cost,
            "failed_items": failed_items,
            "completed_at": datetime.now().isoformat()
        })
        
        # 9. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(combined_file_path):
            os.remove(combined_file_path)
        
        logger.info(f"æµ‹è¯•å®Œæˆ: {task_id}, è´¨é‡åˆ†: {average_score:.3f}")
        
    except Exception as e:
        task.update({
            "status": "failed",
            "error": str(e),
            "completed_at": datetime.now().isoformat()
        })
        logger.error(f"æµ‹è¯•å¤±è´¥: {task_id}, é”™è¯¯: {e}")

async def update_config_file(config: TestConfig):
    """æ›´æ–°config2.yamlæ–‡ä»¶ (åŸºäºä½ çš„é…ç½®æ ¼å¼)"""
    import yaml
    
    config_data = {
        "models": {
            config.llm_config: {
                "api_type": "openai",
                "base_url": config.base_url,
                "api_key": config.api_key,
                "temperature": 0
            },
            config.user_config: {
                "api_type": "openai",
                "base_url": config.base_url, 
                "api_key": config.api_key,
                "temperature": 0
            }
        }
    }
    
    config_dir = Path("config")
    config_dir.mkdir(exist_ok=True)
    
    with open(config_dir / "config2.yaml", 'w', encoding='utf-8') as f:
        yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)

async def merge_uploaded_files(file_ids: List[str], task_id: str) -> str:
    """åˆå¹¶ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶ä¸ºå•ä¸ªJSONLæ–‡ä»¶"""

    output_dir = Path("workspace") / task_id
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / f"temp_combined_{task_id}.jsonl"
    
    with open(output_path, 'w', encoding='utf-8') as out_f:
        for file_id in file_ids:
            file_path = uploaded_files[file_id]
            with open(file_path, 'r', encoding='utf-8') as in_f:
                if file_path.endswith('.jsonl'):
                    for line in in_f:
                        if line.strip():
                            out_f.write(line)
                else:  # .json
                    data = json.load(in_f)
                    if isinstance(data, list):
                        for item in data:
                            out_f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    else:
                        out_f.write(json.dumps(data, ensure_ascii=False) + '\n')
    
    return output_path

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ å¯åŠ¨InteractCompæ ‡æ³¨è´¨é‡æµ‹è¯•å¹³å°APIæœåŠ¡")
    print("ğŸŒ APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ“Š åŸºäºç°æœ‰InteractCompAgentå’ŒInteractCompBenchmark")
    uvicorn.run(app, host="0.0.0.0", port=8000)