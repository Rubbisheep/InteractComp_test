#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆFastAPI - å›ºå®šä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹è¯„ä¼°æ ‡æ³¨è´¨é‡
"""

import asyncio
import json
import os
import tempfile
import uuid
import csv
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import shutil

from fastapi import FastAPI, File, HTTPException, UploadFile, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel

# å¯¼å…¥ç°æœ‰çš„æ¨¡å—
from benchmarks.InteractComp import InteractCompBenchmark
from workflow.InteractComp import InteractCompAgent
from utils.logs import logger

app = FastAPI(title="InteractCompæ ‡æ³¨è´¨é‡æµ‹è¯•API", version="2.0.0")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"],
)

# å›ºå®šçš„ä¸‰ä¸ªè¯„ä¼°æ¨¡å‹
EVALUATION_MODELS = [
    "gpt-5-mini",
    "gpt-5", 
    "claude-4-sonnet"
]

# æ”¯æŒçš„æœç´¢å¼•æ“é…ç½®ï¼ˆç”¨äºGoogleæœç´¢ï¼‰
def get_search_config():
    """è·å–æœç´¢å¼•æ“é…ç½®"""
    try:
        config = get_config()
        return config.get('search', {})
    except:
        return {}

# é…ç½®è¯»å–å‡½æ•°
def load_config():
    """ä»config2.yamlåŠ è½½é…ç½®"""
    import yaml
    from pathlib import Path
    
    config_paths = [
        Path("config/config2.yaml"),
        Path("config2.yaml"),
        Path("./config/config2.yaml")
    ]
    
    for config_path in config_paths:
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
            except Exception as e:
                logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                continue
    
    raise FileNotFoundError("æœªæ‰¾åˆ°config2.yamlé…ç½®æ–‡ä»¶")

# å…¨å±€é…ç½®
CONFIG = None

def get_config():
    """è·å–é…ç½®ï¼Œå¦‚æœæœªåŠ è½½åˆ™åŠ è½½"""
    global CONFIG
    if CONFIG is None:
        CONFIG = load_config()
    return CONFIG

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨
tasks: Dict[str, dict] = {}
uploaded_files: Dict[str, str] = {}

@app.get("/")
async def root():
    """æ ¹è·¯å¾„ - æ£€æŸ¥æœåŠ¡çŠ¶æ€"""
    return {
        "message": "InteractCompæ ‡æ³¨æ•°æ®è´¨é‡æµ‹è¯•API - é…ç½®æ–‡ä»¶ç‰ˆæœ¬",
        "version": "2.1.0",
        "evaluation_models": EVALUATION_MODELS,
        "config_note": "API Keysé€šè¿‡config2.yamlæ–‡ä»¶é…ç½®",
        "status": "running"
    }

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ JSONLæ•°æ®æ–‡ä»¶"""
    if not file.filename.endswith(('.jsonl', '.json')):
        raise HTTPException(status_code=400, detail="åªæ”¯æŒ.jsonlå’Œ.jsonæ ¼å¼æ–‡ä»¶")
    
    file_id = str(uuid.uuid4())
    temp_dir = Path("temp_uploads")
    temp_dir.mkdir(exist_ok=True)
    
    file_path = temp_dir / f"{file_id}_{file.filename}"
    
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    
    uploaded_files[file_id] = str(file_path)
    
    logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file.filename}, ID: {file_id}")
    
    return {
        "file_id": file_id,
        "filename": file.filename,
        "size": file_path.stat().st_size,
        "status": "uploaded"
    }

@app.post("/test/start")
async def start_test(
    file_ids: List[str],
    background_tasks: BackgroundTasks
):
    """å¼€å§‹ä¸‰æ¨¡å‹è¯„ä¼°æµ‹è¯•"""
    # éªŒè¯æ–‡ä»¶
    for file_id in file_ids:
        if file_id not in uploaded_files:
            raise HTTPException(status_code=400, detail=f"æ–‡ä»¶{file_id}ä¸å­˜åœ¨")
    
    # éªŒè¯é…ç½®æ–‡ä»¶
    try:
        config = get_config()
        if 'models' not in config:
            raise HTTPException(status_code=500, detail="é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘modelsé…ç½®")
    except FileNotFoundError:
        raise HTTPException(status_code=500, detail="æœªæ‰¾åˆ°config2.yamlé…ç½®æ–‡ä»¶ï¼Œè¯·å…ˆé…ç½®API Keys")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
    
    # åˆ›å»ºä»»åŠ¡
    task_id = str(uuid.uuid4())
    task_info = {
        "task_id": task_id,
        "status": "pending",
        "progress": 0,
        "created_at": datetime.now().isoformat(),
        "file_ids": file_ids,
        "evaluation_models": EVALUATION_MODELS
    }
    tasks[task_id] = task_info
    
    # å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(run_multi_model_evaluation, task_id, file_ids)
    
    logger.info(f"ä¸‰æ¨¡å‹è¯„ä¼°ä»»åŠ¡å¯åŠ¨: {task_id}")
    
    return {"task_id": task_id, "status": "started", "models": EVALUATION_MODELS}

@app.get("/test/{task_id}")
async def get_test_status(task_id: str):
    """è·å–æµ‹è¯•çŠ¶æ€"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return tasks[task_id]

@app.get("/test/{task_id}/download-csv")
async def download_csv_report(task_id: str):
    """ä¸‹è½½CSVæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
    if task_id not in tasks:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = tasks[task_id]
    if task["status"] != "completed":
        raise HTTPException(status_code=400, detail="æµ‹è¯•æœªå®Œæˆ")
    
    # åˆ›å»ºè¯¦ç»†çš„CSVæŠ¥å‘Š
    import tempfile
    temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False, encoding='utf-8')
    
    writer = csv.writer(temp_file)
    # CSVæ ‡é¢˜
    writer.writerow([
        'question', 'correct_answer', 
        'gpt5_mini_answer', 'gpt5_mini_correct',
        'gpt5_answer', 'gpt5_correct', 
        'claude4_answer', 'claude4_correct',
        'models_correct_count', 'quality_failed', 'cost'
    ])
    
    # å†™å…¥æ•°æ®
    for item in task.get("detailed_results", []):
        writer.writerow([
            item.get("question", ""),
            item.get("correct_answer", ""),
            item.get("model_results", {}).get("gpt-4o-mini", {}).get("answer", ""),
            item.get("model_results", {}).get("gpt-4o-mini", {}).get("correct", False),
            item.get("model_results", {}).get("gpt-4o", {}).get("answer", ""),
            item.get("model_results", {}).get("gpt-4o", {}).get("correct", False),
            item.get("model_results", {}).get("claude-3-5-sonnet-20241022", {}).get("answer", ""),
            item.get("model_results", {}).get("claude-3-5-sonnet-20241022", {}).get("correct", False),
            item.get("correct_models_count", 0),
            item.get("quality_failed", False),
            item.get("total_cost", 0.0)
        ])
    
    temp_file.close()
    
    return FileResponse(
        path=temp_file.name,
        filename=f"ä¸‰æ¨¡å‹è¯„ä¼°æŠ¥å‘Š_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        media_type='text/csv'
    )

async def run_multi_model_evaluation(task_id: str, file_ids: List[str]):
    """è¿è¡Œä¸‰æ¨¡å‹è¯„ä¼°"""
    task = tasks[task_id]
    
    try:
        task["status"] = "running"
        task["progress"] = 10
        
        # 1. è¯»å–é…ç½®æ–‡ä»¶ï¼ˆä¸å†éœ€è¦æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰é…ç½®ï¼‰
        config = get_config()
        task["progress"] = 20
        
        # 2. åˆå¹¶æ•°æ®æ–‡ä»¶
        combined_file_path = await merge_uploaded_files(file_ids, task_id)
        task["progress"] = 30

        # 3. åˆ›å»ºå¤šæ¨¡å‹è¯„ä¼°å™¨å’ŒAgentå·¥å‚
        log_path = f"workspace/multi_model_test_{task_id}/"
        os.makedirs(log_path, exist_ok=True)

        # ä½¿ç”¨ä¿®æ”¹åçš„InteractCompBenchmarkï¼Œæ”¯æŒå¤šæ¨¡å‹è¯„ä¼°
        benchmark = InteractCompBenchmark(
            name=f"MultiModelTest_{task_id}",
            file_path=combined_file_path,
            log_path=log_path,
            grader_config="gpt-4o",
            models=EVALUATION_MODELS  # ä¼ å…¥è¯„ä¼°æ¨¡å‹åˆ—è¡¨
        )

        # åˆ›å»ºAgentå·¥å‚
        from workflow.InteractComp import create_multi_model_agent_factory
        agent_factory = create_multi_model_agent_factory(
            max_turns=5,  # å¤šæ¨¡å‹è¯„ä¼°æ—¶å‡å°‘è½®æ•°èŠ‚çœæˆæœ¬
            search_engine_type="google",
            user_config="gpt-4o"
        )
        task["progress"] = 40
        
        # 4. æ‰§è¡Œå¤šæ¨¡å‹è¯„ä¼°
        logger.info(f"å¼€å§‹å¤šæ¨¡å‹è¯„ä¼°: {task_id}")

        results = await benchmark.run_multi_model_evaluation(
            agent_factory, 
            max_concurrent_tasks=20
        )
        task["progress"] = 90
        
        # 5. å¤„ç†ç»“æœ
        avg_quality_failed_rate = results["avg_quality_failed_rate"]
        total_questions = results["total_questions"]
        quality_failed_count = results["quality_failed_count"]  
        quality_passed_count = total_questions - quality_failed_count
        total_cost = results["total_cost"]
        detailed_results = results["detailed_results"]
        
        # 6. æ›´æ–°ä»»åŠ¡ç»“æœ
        task.update({
            "status": "completed",
            "progress": 100,
            "total_questions": total_questions,
            "quality_passed_count": quality_passed_count,  # è´¨é‡åˆæ ¼æ•°ï¼ˆå°‘æ•°æ¨¡å‹ç­”å¯¹ï¼‰
            "quality_failed_count": quality_failed_count,  # è´¨é‡ä¸åˆæ ¼æ•°ï¼ˆå¤šæ•°æ¨¡å‹ç­”å¯¹ï¼‰
            "quality_failed_rate": avg_quality_failed_rate,  # è´¨é‡ä¸åˆæ ¼ç‡
            "total_cost": total_cost,
            "detailed_results": detailed_results,
            "failed_items": [item for item in detailed_results if item["quality_failed"]],
            "completed_at": datetime.now().isoformat(),
            "evaluation_summary": {
                "models_used": EVALUATION_MODELS,
                "evaluation_logic": "è´¨é‡ä¸åˆæ ¼ = 2ä¸ªä»¥ä¸Šæ¨¡å‹ç­”å¯¹",
                "quality_standard": "ä¼˜ç§€æ ‡æ³¨åº”è¯¥è®©å¤šæ•°AIæ¨¡å‹éš¾ä»¥æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ"
            }
        })
        
        # 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(combined_file_path):
            os.remove(combined_file_path)
        
        logger.info(f"ä¸‰æ¨¡å‹è¯„ä¼°å®Œæˆ: {task_id}, ä¸åˆæ ¼ç‡: {avg_quality_failed_rate:.3f}")
        
    except Exception as e:
        task.update({
            "status": "failed",
            "error": str(e),
            "completed_at": datetime.now().isoformat()
        })
        logger.error(f"ä¸‰æ¨¡å‹è¯„ä¼°å¤±è´¥: {task_id}, é”™è¯¯: {e}")

# å¤šæ¨¡å‹è¯„ä¼°åŸºå‡†æµ‹è¯•ç±»
class MultiModelBenchmark:
    def __init__(self, name: str, file_path: str, log_path: str, models: List[str]):
        self.name = name
        self.file_path = file_path
        self.log_path = log_path  
        self.models = models
        self.grader_llm = None  # å°†åœ¨è¯„ä¼°æ—¶åˆå§‹åŒ–

    async def run_multi_model_evaluation(self, max_concurrent_tasks: int = 20):
        """è¿è¡Œå¤šæ¨¡å‹è¯„ä¼°"""
        # åŠ è½½æ•°æ®
        data = []
        with open(self.file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        
        # åˆå§‹åŒ–è¯„åˆ†å™¨
        from utils.async_llm import AsyncLLM
        self.grader_llm = AsyncLLM("gpt-4o")
        
        # è¯„ä¼°æ‰€æœ‰é—®é¢˜
        detailed_results = []
        total_cost = 0.0
        quality_failed_count = 0
        
        for i, problem in enumerate(data):
            logger.info(f"è¯„ä¼°é—®é¢˜ {i+1}/{len(data)}")
            
            result = await self.evaluate_single_problem(problem)
            detailed_results.append(result)
            total_cost += result["total_cost"]
            
            if result["quality_failed"]:
                quality_failed_count += 1
        
        avg_quality_failed_rate = quality_failed_count / len(data) if data else 0
        
        return {
            "total_questions": len(data),
            "quality_failed_count": quality_failed_count,
            "avg_quality_failed_rate": avg_quality_failed_rate,
            "total_cost": total_cost,
            "detailed_results": detailed_results
        }

    async def evaluate_single_problem(self, problem: dict):
        """è¯„ä¼°å•ä¸ªé—®é¢˜"""
        question = problem["question"]
        correct_answer = problem.get("answer", "")
        
        model_results = {}
        total_cost = 0.0
        correct_models_count = 0
        
        # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
        for model in self.models:
            try:
                # åˆ›å»ºAgent
                agent = InteractCompAgent(
                    name=f"Agent_{model}",
                    llm_config=model,
                    dataset="InteractComp", 
                    prompt="",
                    max_turns=5,  # å‡å°‘è½®æ•°èŠ‚çœæˆæœ¬
                    search_engine_type="google",
                    user_config="gpt-4o"
                )
                
                # è·å–æ¨¡å‹ç­”æ¡ˆ
                predicted_answer, history, cost = await agent(problem)
                total_cost += cost
                
                # è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§
                is_correct = await self.grade_answer(question, correct_answer, predicted_answer)
                
                model_results[model] = {
                    "answer": predicted_answer,
                    "correct": is_correct,
                    "cost": cost,
                    "history_length": len(history)
                }
                
                if is_correct:
                    correct_models_count += 1
                    
                logger.info(f"æ¨¡å‹ {model}: {'æ­£ç¡®' if is_correct else 'é”™è¯¯'}")
                    
            except Exception as e:
                logger.error(f"æ¨¡å‹ {model} è¯„ä¼°å¤±è´¥: {e}")
                model_results[model] = {
                    "answer": "è¯„ä¼°å¤±è´¥",
                    "correct": False,
                    "cost": 0.0,
                    "error": str(e)
                }
        
        # åˆ¤æ–­è´¨é‡ï¼š2ä¸ªä»¥ä¸Šæ¨¡å‹ç­”å¯¹å°±æ˜¯è´¨é‡ä¸åˆæ ¼
        quality_failed = correct_models_count >= 2
        
        return {
            "question": question,
            "correct_answer": correct_answer,
            "model_results": model_results,
            "correct_models_count": correct_models_count,
            "quality_failed": quality_failed,
            "total_cost": total_cost
        }

    async def grade_answer(self, question: str, correct_answer: str, predicted_answer: str) -> bool:
        """è¯„ä¼°ç­”æ¡ˆæ­£ç¡®æ€§"""
        grading_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è¯„åˆ†å‘˜ã€‚

é—®é¢˜: {question}
é¢„æµ‹ç­”æ¡ˆ: {predicted_answer}
æ­£ç¡®ç­”æ¡ˆ: {correct_answer}

å…³é”®è¯„åˆ†æŒ‡ä»¤:
1. é¢„æµ‹ç­”æ¡ˆå¿…é¡»ä¸æ­£ç¡®ç­”æ¡ˆåŒ¹é…
2. å¯»æ‰¾ç¡®åˆ‡çš„åç§°åŒ¹é…æˆ–å¯¹åŒä¸€å®ä½“çš„æ˜ç¡®å¼•ç”¨
3. è€ƒè™‘ä¸åŒè¯­è¨€ã€ç¿»è¯‘æˆ–æ›¿ä»£åç§°ä½œä¸ºæ½œåœ¨åŒ¹é…
4. ä¸¥æ ¼è¦æ±‚ï¼šéƒ¨åˆ†åŒ¹é…æˆ–æ¨¡ç³Šç›¸ä¼¼æ€§åº”ä¸º'no'

é‡è¦ï¼šåªç»™å‡ºä¸€ä¸ªè¯„åˆ†ï¼š
- 'yes': é¢„æµ‹ç­”æ¡ˆæ­£ç¡®è¯†åˆ«äº†ä¸æ­£ç¡®ç­”æ¡ˆç›¸åŒçš„å®ä½“
- 'no': é¢„æµ‹ç­”æ¡ˆé”™è¯¯ã€åŒ¹é…äº†æµè¡Œç­”æ¡ˆï¼Œæˆ–æŒ‡å‘äº†ä¸åŒçš„å®ä½“

åªå›ç­”'yes'æˆ–'no'ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

        try:
            response = await self.grader_llm(grading_prompt)
            
            if "yes" in response.strip().lower():
                return True
            elif "no" in response.strip().lower():
                return False
            else:
                return False
                
        except Exception as e:
            logger.error(f"è¯„åˆ†å¤±è´¥: {e}")
            return False

@app.get("/config/status")
async def get_config_status():
    """æ£€æŸ¥é…ç½®æ–‡ä»¶çŠ¶æ€"""
    try:
        config = get_config()
        models = config.get('models', {})
        
        # æ£€æŸ¥å¿…éœ€çš„æ¨¡å‹é…ç½®
        required_models = EVALUATION_MODELS
        missing_models = []
        configured_models = []
        
        for model in required_models:
            if model not in models:
                missing_models.append(f"{model} (æœªé…ç½®)")
            elif not models[model].get('api_key'):
                missing_models.append(f"{model} (ç¼ºå°‘API Key)")
            else:
                configured_models.append(model)
        
        logger.info(f"é…ç½®æ£€æŸ¥ - å·²é…ç½®: {configured_models}, ç¼ºå¤±: {missing_models}")
        
        return {
            "config_found": True,
            "models_configured": len(configured_models),
            "configured_models": configured_models,
            "required_models": required_models,
            "missing_models": missing_models,
            "ready": len(missing_models) == 0,
            "config_path": "config/config2.yaml"
        }
        
    except FileNotFoundError:
        logger.warning("é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°")
        return {
            "config_found": False,
            "error": "æœªæ‰¾åˆ° config2.yaml é…ç½®æ–‡ä»¶",
            "ready": False,
            "suggestion": "è¯·å‚è€ƒ config2.example.yaml åˆ›å»ºé…ç½®æ–‡ä»¶"
        }
    except Exception as e:
        logger.error(f"é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        return {
            "config_found": False,
            "error": f"é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}",
            "ready": False
        }

async def merge_uploaded_files(file_ids: List[str], task_id: str) -> str:
    """åˆå¹¶ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶ä¸ºå•ä¸ªJSONLæ–‡ä»¶"""
    output_dir = Path("workspace") / task_id
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / f"temp_combined_{task_id}.jsonl"
    
    with open(output_path, 'w', encoding='utf-8') as out_f:
        for file_id in file_ids:
            file_path = uploaded_files[file_id]
            with open(file_path, 'r', encoding='utf-8') as in_f:
                if file_path.endswith('.jsonl'):
                    for line in in_f:
                        if line.strip():
                            out_f.write(line)
                else:  # .json
                    data = json.load(in_f)
                    if isinstance(data, list):
                        for item in data:
                            out_f.write(json.dumps(item, ensure_ascii=False) + '\n')
                    else:
                        out_f.write(json.dumps(data, ensure_ascii=False) + '\n')
    
    return str(output_path)

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸš€ å¯åŠ¨InteractCompä¸‰æ¨¡å‹æ ‡æ³¨è´¨é‡æµ‹è¯•å¹³å°")
    print("ğŸ¤– è¯„ä¼°æ¨¡å‹:", EVALUATION_MODELS)
    print("ğŸ“Š è¯„ä¼°é€»è¾‘: 2ä¸ªä»¥ä¸Šæ¨¡å‹ç­”å¯¹ = æ ‡æ³¨è´¨é‡ä¸åˆæ ¼")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    try:
        config = load_config()
        print("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        models = config.get('models', {})
        
        for model in EVALUATION_MODELS:
            if model in models and models[model].get('api_key'):
                print(f"âœ… {model} é…ç½®å®Œæˆ")
            else:
                print(f"âš ï¸ {model} é…ç½®ç¼ºå¤±")
                
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°config2.yamlé…ç½®æ–‡ä»¶")
        print("ğŸ“ è¯·å‚è€ƒconfig2.example.yamlåˆ›å»ºé…ç½®æ–‡ä»¶")
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    
    print("ğŸŒ APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ“‹ é…ç½®çŠ¶æ€: http://localhost:8000/config/status")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)