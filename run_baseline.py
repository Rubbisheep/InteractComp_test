#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@File    :   run_baseline.py
@Time    :   2025/08/11
@Author  :   Deng Mingyi1
"""

import asyncio
from utils.logs import logger

from benchmarks.InteractComp import InteractCompBenchmark
from workflow.InteractComp import InteractCompAgent, create_multi_model_agent_factory

# è¯„ä¼°æ¨¡å‹é…ç½®
SINGLE_MODEL_CONFIGS = [
    "gpt-5-mini",
    "gpt-5", 
    "claude-4-sonnet"
]

MULTI_MODEL_CONFIGS = [
    "gpt-5-mini",
    "gpt-5", 
    "claude-4-sonnet"
]

async def run_single_model_evaluation():
    """è¿è¡Œå•æ¨¡å‹è¯„ä¼°ï¼ˆåŸæœ‰æ¨¡å¼ï¼‰"""
    dataset_path = "data/data.jsonl"
    log_path = "workspace/"
    prompt = ""
    
    results_summary = []
    
    for model_config in SINGLE_MODEL_CONFIGS:
        print(f"\nğŸš€ Running single model evaluation: {model_config}")
        
        # åˆ›å»ºå•ä¸ªAgent
        agent = InteractCompAgent(
            name=f"SingleAgent_{model_config}",
            llm_config=model_config,
            dataset="InteractComp",
            prompt=prompt,
            max_turns=5,
            search_engine_type="google",
            user_config="gpt-4o"
        )

        # åˆ›å»ºå•æ¨¡å‹åŸºå‡†æµ‹è¯•
        benchmark = InteractCompBenchmark(
            name=f"SingleModel_{model_config}", 
            file_path=dataset_path, 
            log_path=f"{log_path}/single_{model_config}/",
            grader_config="gpt-4o"
            # ä¸ä¼ å…¥modelså‚æ•°ï¼Œä½¿ç”¨å•æ¨¡å‹æ¨¡å¼
        )

        # è¿è¡Œè¯„ä¼°
        avg_score, avg_cost, total_cost = await benchmark.run_baseline(agent, max_concurrent_tasks=1)
        
        results_summary.append({
            "model": model_config,
            "mode": "single",
            "avg_score": avg_score,
            "avg_cost": avg_cost,
            "total_cost": total_cost
        })
        
        print(f"ğŸ¯ {model_config} - Average Score: {avg_score:.3f} ({avg_score*100:.1f}%)")
        print(f"ğŸ’° {model_config} - Average Cost: ${avg_cost:.4f}")
        print(f"ğŸ’° {model_config} - Total Cost: ${total_cost:.4f}")
    
    return results_summary

async def run_multi_model_evaluation():
    """è¿è¡Œå¤šæ¨¡å‹è¯„ä¼°ï¼ˆæ–°æ¨¡å¼ï¼‰"""
    dataset_path = "data/data.jsonl"
    log_path = "workspace/multi_model/"
    
    print(f"\nğŸ¤– Running multi-model evaluation: {MULTI_MODEL_CONFIGS}")
    
    # åˆ›å»ºAgentå·¥å‚
    agent_factory = create_multi_model_agent_factory(
        max_turns=5,  # å¤šæ¨¡å‹æ—¶å‡å°‘è½®æ•°èŠ‚çœæˆæœ¬
        search_engine_type="google",
        user_config="gpt-4o"
    )

    # åˆ›å»ºå¤šæ¨¡å‹åŸºå‡†æµ‹è¯•
    benchmark = InteractCompBenchmark(
        name="MultiModelEvaluation", 
        file_path=dataset_path, 
        log_path=log_path,
        grader_config="gpt-4o",
        models=MULTI_MODEL_CONFIGS  # ä¼ å…¥æ¨¡å‹åˆ—è¡¨å¯ç”¨å¤šæ¨¡å‹æ¨¡å¼
    )

    # è¿è¡Œå¤šæ¨¡å‹è¯„ä¼°
    results = await benchmark.run_multi_model_evaluation(
        agent_factory, 
        max_concurrent_tasks=1
    )
    
    print(f"\nğŸ¯ Multi-Model Evaluation Results:")
    print(f"ğŸ“Š Total Questions: {results['total_questions']}")
    print(f"âŒ Quality Failed: {results['quality_failed_count']} ({results['avg_quality_failed_rate']*100:.1f}%)")
    print(f"âœ… Quality Passed: {results['total_questions'] - results['quality_failed_count']} ({(1-results['avg_quality_failed_rate'])*100:.1f}%)")
    print(f"ğŸ’° Total Cost: ${results['total_cost']:.4f}")
    print(f"ğŸ’° Average Cost: ${results['avg_cost']:.4f}")
    
    # æ˜¾ç¤ºè´¨é‡ä¸åˆæ ¼çš„é—®é¢˜
    failed_items = [item for item in results['detailed_results'] if item['quality_failed']]
    if failed_items:
        print(f"\nâš ï¸  Quality Failed Items (first 5):")
        for i, item in enumerate(failed_items[:5], 1):
            print(f"{i}. Q: {item['question'][:100]}...")
            print(f"   Correct models: {item['correct_models_count']}/{len(MULTI_MODEL_CONFIGS)}")
    
    return results

async def run_comparison():
    """è¿è¡Œå•æ¨¡å‹vså¤šæ¨¡å‹å¯¹æ¯”è¯„ä¼°"""
    print("=" * 80)
    print("ğŸ”¬ InteractComp Evaluation - Single Model vs Multi Model Comparison")
    print("=" * 80)
    
    # è¿è¡Œå•æ¨¡å‹è¯„ä¼°
    print("\nğŸ“‹ Phase 1: Single Model Evaluations")
    single_results = await run_single_model_evaluation()
    
    # è¿è¡Œå¤šæ¨¡å‹è¯„ä¼°
    print("\nğŸ“‹ Phase 2: Multi Model Evaluation")
    multi_results = await run_multi_model_evaluation()
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š COMPARISON RESULTS")
    print("=" * 80)
    
    print("\nğŸ” Single Model Results:")
    total_single_cost = 0
    for result in single_results:
        print(f"  {result['model']}: Score={result['avg_score']:.3f}, Cost=${result['total_cost']:.4f}")
        total_single_cost += result['total_cost']
    
    print(f"\nğŸ¤– Multi Model Results:")
    print(f"  Quality Failed Rate: {multi_results['avg_quality_failed_rate']:.3f}")
    print(f"  Total Cost: ${multi_results['total_cost']:.4f}")
    
    print(f"\nğŸ’° Cost Comparison:")
    print(f"  Single Models Total: ${total_single_cost:.4f}")
    print(f"  Multi Model: ${multi_results['total_cost']:.4f}")
    print(f"  Cost Ratio: {multi_results['total_cost']/total_single_cost:.2f}x")
    
    print("\nâœ¨ Evaluation Logic:")
    print("  Single Model: Model wrong = Good annotation quality")  
    print("  Multi Model: 2+ models correct = Poor annotation quality")
    
    return {
        "single_results": single_results,
        "multi_results": multi_results
    }

async def main():
    """ä¸»å‡½æ•° - æ”¯æŒä¸åŒçš„è¯„ä¼°æ¨¡å¼"""
    import sys
    
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
    else:
        mode = "multi"  # é»˜è®¤ä½¿ç”¨å¤šæ¨¡å‹è¯„ä¼°
    
    if mode == "single":
        print("ğŸ”¬ Running Single Model Evaluation Mode")
        return await run_single_model_evaluation()
    elif mode == "multi":
        print("ğŸ”¬ Running Multi Model Evaluation Mode") 
        return await run_multi_model_evaluation()
    elif mode == "comparison":
        print("ğŸ”¬ Running Comparison Mode")
        return await run_comparison()
    else:
        print("âŒ Unknown mode. Use: single, multi, or comparison")
        print("Usage: python run_baseline.py [single|multi|comparison]")
        return None

if __name__ == "__main__":
    asyncio.run(main())